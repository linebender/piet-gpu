// This is "kernel 2" (fill) in a 4-kernel pipeline. It processes the fill
// (polyline) items in the scene and generates a list of segments for each, for
// each tile.

#version 450
#extension GL_GOOGLE_include_directive : enable

layout(local_size_x = 32) in;

layout(set = 0, binding = 0) readonly buffer SceneBuf {
    uint[] scene;
};

layout(set = 0, binding = 1) buffer TilegroupBuf {
    uint[] tilegroup;
};

layout(set = 0, binding = 2) buffer FillSegBuf {
    uint[] fill_seg;
};

layout(set = 0, binding = 3) buffer AllocBuf {
    uint alloc;
};

#include "scene.h"
#include "tilegroup.h"
#include "fill_seg.h"

#include "setup.h"

// Factor into a separate .h file to avoid code duplication?

// We should be able to use gl_SubgroupSize, but that's problematic.
#define SUBGROUP_SIZE 32

shared vec2 sh_start[SUBGROUP_SIZE];
shared vec2 sh_end[SUBGROUP_SIZE];
shared int backdrop[SUBGROUP_SIZE];
shared uint intersects[SUBGROUP_SIZE];

// Ensure that there is space to encode a segment.
void alloc_chunk(inout uint chunk_n_segs, inout FillSegChunkRef seg_chunk_ref,
    inout FillSegChunkRef first_seg_chunk, inout uint seg_limit)
{
    if (chunk_n_segs == 0) {
        if (seg_chunk_ref.offset + 40 > seg_limit) {
            seg_chunk_ref.offset = atomicAdd(alloc, SEG_CHUNK_ALLOC);
            seg_limit = seg_chunk_ref.offset + SEG_CHUNK_ALLOC - FillSegment_size;
        }
        first_seg_chunk = seg_chunk_ref;
    } else if (seg_chunk_ref.offset + FillSegChunk_size + FillSegment_size * chunk_n_segs > seg_limit) {
        uint new_chunk_ref = atomicAdd(alloc, SEG_CHUNK_ALLOC);
        seg_limit = new_chunk_ref + SEG_CHUNK_ALLOC - FillSegment_size;
        FillSegChunk_write(seg_chunk_ref, FillSegChunk(chunk_n_segs, FillSegChunkRef(new_chunk_ref)));
        seg_chunk_ref.offset = new_chunk_ref;
        chunk_n_segs = 0;
    }

}

void main() {
    uint tile_ix = gl_GlobalInvocationID.y * WIDTH_IN_TILES + gl_GlobalInvocationID.x;
    uint tilegroup_ix = gl_GlobalInvocationID.y * WIDTH_IN_TILEGROUPS
        + (gl_GlobalInvocationID.x / TILEGROUP_WIDTH_TILES);
    float blockx0 = float((gl_GlobalInvocationID.x / TILEGROUP_WIDTH_TILES) * TILEGROUP_WIDTH_PX);
    vec2 xy0 = vec2(gl_GlobalInvocationID.xy) * vec2(TILE_WIDTH_PX, TILE_HEIGHT_PX);
    TileGroupRef fill_start = TileGroupRef(tilegroup_ix * TILEGROUP_STRIDE + TILEGROUP_FILL_START);
    uint fill_n = tilegroup[fill_start.offset >> 2];

    FillTileHeaderRef tile_header_ref = FillTileHeaderRef(tile_ix * FillTileHeader_size);
    if (fill_n > 0) {
        ChunkRef chunk_ref = ChunkRef(fill_start.offset + 4);
        Chunk chunk = Chunk_read(chunk_ref);
        InstanceRef fill_ref = InstanceRef(chunk_ref.offset + Chunk_size);
        FillItemHeaderRef item_header = FillItemHeaderRef(atomicAdd(alloc, fill_n * FillItemHeader_size));
        FillTileHeader_write(tile_header_ref, FillTileHeader(fill_n, item_header));
        FillSegChunkRef seg_chunk_ref = FillSegChunkRef(0);
        uint seg_limit = 0;
 
        // Iterate through items; chunk.chunk_n holds count remaining.
        while (true) {
            if (chunk.chunk_n == 0) {
                chunk_ref = chunk.next;
                if (chunk_ref.offset == 0) {
                    break;
                }
                chunk = Chunk_read(chunk_ref);
                fill_ref = InstanceRef(chunk_ref.offset + Chunk_size);
            }
            Instance ins = Instance_read(fill_ref);
            PietFill fill = PietItem_Fill_read(PietItemRef(ins.item_ref));
            uint n = fill.n_points - 1;

            uint chunk_n_segs = 0;
            backdrop[gl_LocalInvocationID.x] = 0;
            FillSegChunkRef first_seg_chunk = FillSegChunkRef(0);
            for (uint j0 = 0; j0 < n; j0 += SUBGROUP_SIZE) {
                uint j = j0 + gl_LocalInvocationID.x;
                intersects[gl_LocalInvocationID.x] = 0;
                barrier();
                if (j < n) {
                    vec2 start = Point_read(Point_index(fill.points, j)).xy;
                    vec2 end = Point_read(Point_index(fill.points, j + 1)).xy;
                    sh_start[gl_LocalInvocationID.x] = start;
                    sh_end[gl_LocalInvocationID.x] = end;
                    float y0 = max(xy0.y, min(start.y, end.y));
                    float y1 = min(xy0.y + float(TILE_HEIGHT_PX), max(start.y, end.y));
                    if (y0 <= y1) {
                        float dy_inv = 1.0 / (end.y - start.y);
                        // Maybe fuzzy equality? There might also be a fancier way to do this
                        // with NaN behavior in min and max, but that sounds potentially flaky.
                        float u0 = start.y == end.y ? 0.0 : clamp((y0 - start.y) * dy_inv, 0.0, 1.0);
                        float u1 = start.y == end.y ? 1.0 : clamp((y1 - start.y) * dy_inv, 0.0, 1.0);
                        // maybe better to clamp after the mix (more precision)?
                        float x0 = mix(start.x, end.x, u0);
                        float x1 = mix(start.x, end.x, u1);
                        float xmin = max(min(x0, x1) - blockx0, 0.0);
                        float xmax = clamp(max(x0, x1) - blockx0, 0.0, float(TILEGROUP_WIDTH_PX));

                        uint x0i = uint(floor(xmin * (1.0 / TILE_WIDTH_PX)));
                        uint x1i = uint(ceil(xmax * (1.0 / TILE_WIDTH_PX)));
                        for (uint ix = x0i; ix < x1i; ix++) {
                            atomicOr(intersects[ix], 1 << gl_LocalInvocationID.x);
                        }

                        // backdrop calculation
                        if (y0 == xy0.y && y0 < y1) {
                            float u = (xy0.y - start.y) * dy_inv;
                            float xray = mix(start.x, end.x, u) - blockx0;
                            uint xrayi = uint(ceil(xray * (1.0 / TILE_WIDTH_PX)));
                            if (xrayi < 32) {
                                int delta = end.y < start.y ? 1 : -1;
                                atomicAdd(backdrop[xrayi], delta);
                            }
                        }
                    }
                }

                barrier();

                uint bitmask = intersects[gl_LocalInvocationID.x];
                while (bitmask != 0) {
                    uint k = findLSB(bitmask);
                    alloc_chunk(chunk_n_segs, seg_chunk_ref, first_seg_chunk, seg_limit);
                    vec2 start = sh_start[k];
                    vec2 end = sh_end[k];
                    float yEdge = mix(start.y, end.y, (xy0.x - start.x) / (end.x - start.x));
                    if (min(start.x, end.x) < xy0.x && yEdge >= xy0.y && yEdge < xy0.y + float(TILE_HEIGHT_PX)) {
                        FillSegment edge_seg;
                        if (start.x > end.x) {
                            end = vec2(xy0.x, yEdge);
                            edge_seg.start = end;
                            edge_seg.end = vec2(xy0.x, xy0.y + float(TILE_HEIGHT_PX));
                        } else {
                            start = vec2(xy0.x, yEdge);
                            edge_seg.start = vec2(xy0.x, xy0.y + float(TILE_HEIGHT_PX));
                            edge_seg.end = start;
                        }
                        FillSegment_write(FillSegmentRef(seg_chunk_ref.offset + FillSegChunk_size + FillSegment_size * chunk_n_segs), edge_seg);
                        chunk_n_segs++;
                        alloc_chunk(chunk_n_segs, seg_chunk_ref, first_seg_chunk, seg_limit);
                    }
                    FillSegment seg = FillSegment(start, end);
                    FillSegment_write(FillSegmentRef(seg_chunk_ref.offset + FillSegChunk_size + FillSegment_size * chunk_n_segs), seg);
                    chunk_n_segs++;

                    bitmask &= bitmask - 1; // clear bottom bit
                }

            }

            // This is a simple inclusive prefix sum over backdrop.
            // TODO: use subgroups as available (maybe hybrid)
            int my_backdrop = backdrop[gl_LocalInvocationID.x];
            for (int i = 0; i < 5; i++) {
                my_backdrop += gl_LocalInvocationID.x >= (1 << i) ? backdrop[gl_LocalInvocationID.x - (1 << i)] : 0;
                barrier();
                // Last write is not necessary here...
                backdrop[gl_LocalInvocationID.x] = my_backdrop;
                barrier();
            }

            FillItemHeader_write(item_header, FillItemHeader(my_backdrop, first_seg_chunk));
            if (chunk_n_segs != 0) {
                FillSegChunk_write(seg_chunk_ref, FillSegChunk(chunk_n_segs, FillSegChunkRef(0)));
                seg_chunk_ref.offset += FillSegChunk_size + FillSegment_size * chunk_n_segs;
            }

            fill_ref.offset += Instance_size;
            chunk.chunk_n--;
            item_header.offset += FillItemHeader_size;
        }
    } else {
        // As an optimization, we could just write 0 for the size.
        FillTileHeader_write(tile_header_ref, FillTileHeader(fill_n, FillItemHeaderRef(0)));
    }
}
